# -*- coding: utf-8 -*-
"""LVADSUSR156-Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xWUiipl7jQUjqy63fLrmTxwVbpuK_4vC

**CLASSIFICATION MODEL - RANDOM FOREST CLASSIFIER - MUSHROOM CLASS**

**1.1. DATA LOADING**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("//content/mushroom.csv")

df.head()

df.shape

df.info()

df.columns

df.nunique()

df['class'].value_counts()

df.describe(include="all")

"""**1.2. BASIC ANALYSIS - EDA**"""

# Univariate Analysis between the numerical features by plotting Histogram

for num_col in df.select_dtypes(include=['float64', 'int64']).columns:
    sns.histplot(df[num_col], kde=True)
    plt.title(f'Histogram of {num_col}')
    plt.xlabel(num_col)
    plt.ylabel('Frequency')
    plt.show()

## Bi-variate Analysis between numerical features using scatter plots

numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
for i in range(len(numerical_columns)):
    for j in range(i + 1, len(numerical_columns)):
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=df, x=numerical_columns[i], y=numerical_columns[j])
        plt.title(f'Scatter Plot between {numerical_columns[i]} and {numerical_columns[j]}')
        plt.show()

## Bi-variate Analysis between numerical features using correlation matrix and heat map

correlation_matrix = df.corr(numeric_only=True)

correlation_matrix

plt.figure(figsize=(12, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='viridis')
plt.show()

"""**1.3. HANDLING NULL VALUES AND DUPLICATES**"""

df.isnull().sum()

df.head()

df.columns

## For the numerical values we fill the null and missing values help of Simple Imputation with Mean
## because the data is normally distributed

df['stem-color'].fillna(df['stem-color'].mean(), inplace=True)

df.isnull().sum()

df.duplicated().sum()

df.shape

df.drop_duplicates(inplace=True)
df.head()

# There are about 303 duplicated row in the given dataset
# They are repeated rows so we are removing those rows to train our model

df.duplicated().sum()

df.shape

# After removing those duplicates we get these total number of rows to improve the efficiency of our model

"""**1.4. MANAGING OUTLIERS**"""

## Finding the outliers using Box-plot for the numerical features

for col in df.select_dtypes(include=['float','int']):
  sns.boxplot(df[col])
  plt.show()

# Function to calculate the IQR bounds
def calculate_bounds(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return lower_bound, upper_bound

# Function to treat outliers by capping
def treat_outliers(column):
    lower_bound, upper_bound = calculate_bounds(column)
    return column.apply(lambda x: lower_bound if x < lower_bound else upper_bound if x > upper_bound else x)

df.columns

# Removing the outliers which are not relevant to the data set and which can affect the model

for col in df.select_dtypes(include=['float','int']):
  df[col] = treat_outliers(df[col])

# Data after removing outliers

for col in df.select_dtypes(include=['float','int']):
  sns.boxplot(df[col])
  plt.show()

"""**2.1. DATA PREPROCESSING - FEATURE ENGINEERING**"""

df.head()

df.columns

# All the features are necessary to classify the mushrooms which are to be predicted by the model

"""**2.2 ENCODING**"""

df.info()

"""No Encoding is needed because there's no categorical column in the given data set."""

df.head()

cor = df.corr()

plt.figure(figsize=(12, 12))
sns.heatmap(cor, annot=True, cmap='viridis')
plt.show()

"""**3. FEATURE SELECTION**"""

df.columns

## All the features are required for Training the model which helps to predict the class of the mushroom
## So, we are training x with all the features except Target variable class and Y with the Target variable class

x = df.drop(['class'], axis=1)
y = df['class']

"""**4. TRAIN TEST SPLIT**"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 42)

"""**4.1 SCALING USING MIN-MAX SCALAR**"""

mn = MinMaxScaler()
x_train = mn.fit_transform(x_train)
x_test = mn.transform(x_test)

"""**5. MODEL IMPLEMENTATION - RANDOM FOREST**"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)

model.fit(x_train, y_train)

y_pred = model.predict(x_test)

"""**6. MODEL EVALUATION USING METRICS**"""

from sklearn.metrics import accuracy_score, classification_report, precision_score, confusion_matrix, ConfusionMatrixDisplay

acc = accuracy_score(y_test, y_pred)
cr = classification_report(y_test, y_pred)
ps = precision_score(y_test, y_pred)

print("Accuracy Score: ", acc)
print("Precision Score: ", ps)
print("Classification Report: \n", cr)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))

sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**BUSINESS RECOMMENDATIONS**"""

# The model exhibits an overall accuracy of 98.94%, demonstrating its effectiveness
# in correctly predicting  Class of the mushroom.

# From the above model, it's efficiency helps to predict the class of the mushroom
# from which splits the mushroom with more accuracy.