# -*- coding: utf-8 -*-
"""LVADSUSR156-Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IDYawWQFzvdrHDWeaza6FEOmzfKiPblB

**REGRESSION MODEL - LINEAR REGRESSION - BENGALURU HOUSE PRICES**

**1.1. DATA LOADING**
"""

import pandas as pd
import numpy as np
import datetime

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error

df = pd.read_csv("/content/bengaluru_house_prices.csv")

df.head()

df.shape

df.info()

df.columns

"""**CHANGING FEATURES TO THE CORRECT DATATYPES**"""

# Object to  Numeric
df['total_sqft'] = pd.to_numeric(df['total_sqft'],errors='coerce')

df.info()

df.head()

df.describe(include="all")

"""**1.2. BASIC ANALYSIS - EDA**"""

# Univariate Analysis between the numerical features by plotting Histogram

for num_col in df.select_dtypes(include=['float64', 'int64']).columns:
    sns.histplot(df[num_col], kde=True)
    plt.title(f'Histogram of {num_col}')
    plt.xlabel(num_col)
    plt.ylabel('Frequency')
    plt.show()

# Univariate Analysis between the categorical features by plotting Bar Charts

for cat_col in df.select_dtypes(include=['object']).columns:

    sns.barplot(df[cat_col])
    plt.title(f'Bar Chart of {cat_col}')
    plt.xlabel(cat_col)
    plt.ylabel('Count')
    plt.show()

## Bi-variate Analysis between numerical features using scatter plots

numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
for i in range(len(numerical_columns)):
    for j in range(i + 1, len(numerical_columns)):
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=df, x=numerical_columns[i], y=numerical_columns[j])
        plt.title(f'Scatter Plot between {numerical_columns[i]} and {numerical_columns[j]}')
        plt.show()

## Bi-variate Analysis between numerical features using correlation matrix and heat map

correlation_matrix = df.corr(numeric_only=True)

correlation_matrix

plt.figure(figsize=(8, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='viridis')
plt.show()

"""**1.3. HANDLING NULL VALUES AND DUPLICATES**"""

df.isnull().sum()

df.shape

df.nunique()

df.head()

df.columns

##  For the categorical values we fill the null and missing values help of Simple Imputation with Mode

df['location'].fillna(df['location'].mode()[0], inplace=True)
df['size'].fillna(df['size'].mode()[0], inplace=True)
df['society'].fillna(df['society'].mode()[0], inplace=True)
df['size'].fillna(df['size'].mode()[0], inplace=True)

##  For the numerical values we fill the null and missing values help of Simple Imputation with Mean or Median

## For the below features we fill the Null values with Median because the feature is Right Skewed

df['total_sqft'].fillna(df['total_sqft'].median(), inplace=True)
df['bath'].fillna(df['bath'].median(), inplace=True)

## For the below features we fill the Null values with Median because the feature is Right Skewed

df['balcony'].fillna(df['balcony'].mean(), inplace=True)

df.isnull().sum()

df.duplicated().sum()

df.shape

# There are about 530 duplicated row in the given dataset
# They are repeated rows so we are removing those rows to train our model

df.drop_duplicates(inplace=True)
df.head()

df.duplicated().sum()

df.shape

# After removing those duplicates we get these total number of rows to improve the efficiency of our model

"""**1.4. MANAGING OUTLIERS**"""

## Finding the outliers using Box-plot for the numerical features

for col in df.select_dtypes(include=['float','int']):
  sns.boxplot(df[col])
  plt.show()

# Function to calculate the IQR bounds
def calculate_bounds(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return lower_bound, upper_bound

# Function to treat outliers by capping
def treat_outliers(column):
    lower_bound, upper_bound = calculate_bounds(column)
    return column.apply(lambda x: lower_bound if x < lower_bound else upper_bound if x > upper_bound else x)

df.columns

# From the above Boxplot we found that features total_sqft, bath and price
# So, We are removing these outliers which are not relevant to the data set and which can affect the model
# These features like total_sqft, bath, and price are beyond the normal distributed values
# These are treated by IQR function to reduce the outliers

df['total_sqft'] = treat_outliers(df['total_sqft'])
df['bath'] = treat_outliers(df['bath'])
df['price'] = treat_outliers(df['price'])

"""**2.1. DATA PREPROCESSING - FEATURE ENGINEERING**"""

df.columns

df.head()

df.info()

"""**2.2 ENCODING**"""

## We are encoding the following features from categorical to numerical with the help of Label Encoding

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

le = LabelEncoder()

for col in df.select_dtypes(include=["object"]).columns:
  df[col] = le.fit_transform(df[col])

df.head()

cor = df.corr()

plt.figure(figsize=(8, 8))
sns.heatmap(cor, annot=True, cmap='viridis')
plt.show()

"""**3. FEATURE SELECTION**"""

df.columns

## All the features are required for Training the model which helps to predict the Price of the House
## So, we are training x with all the features except Target variable Price and Y with the Target variable Price

x = df.drop(['price'], axis=1)
y = df['price']

"""**4. TRAIN TEST SPLIT**"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

"""**4.1 SCALING USING MIN-MAX SCALER**"""

df.describe(include="all")

## We are using Min-Max Scaler to train the model with trained values

mn = MinMaxScaler()
x_train = mn.fit_transform(x_train)
x_test = mn.transform(x_test)

"""**5. MODEL IMPLEMENTATION - LINEAR REGRESSION**"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(x_train, y_train)

y_pred = model.predict(x_test)

"""**6. MODEL EVALUATION USING METRICS**"""

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)


print("R-squared:", r2)
print("mse:", mse)
print("mae:", mae)

plt.scatter(y_test, y_pred, c='red', label='Actual')
plt.scatter(y_test, y_test, c='blue', label='Predicted')
plt.xlabel("Actual values")
plt.ylabel("Predicted values")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='-', label='bestfitline')
plt.legend()

"""**BUSINESS RECOMMENDATIONS**"""

# From the above Linear Regression Model, it's predicted that the values lies in Best Fit are the predicted
# with the help of train and test

# It's predicted that the price of the Bengaluru Houses rely on these factors and whenever the factors multiple
# the price increases.