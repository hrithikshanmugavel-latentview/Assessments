# -*- coding: utf-8 -*-
"""Clustering-Final-Assessment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hwLh-4RBtvZwgGO5l9fA0DUTcCfcyHuK
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")

from imblearn.over_sampling import SMOTE
from imblearn.metrics import classification_report_imbalanced

from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.cluster import KMeans

df = pd.read_csv("/content/customer_segmentation.csv")
df.head()

## Data Preprocessing

df.shape

df.info()

df.describe(include="all").T

df.drop('ID',axis=1, inplace=True)

df.info()

df.isnull().sum()

df['Income'].fillna(df['Income'].mean(), inplace=True)

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.head()

df.duplicated().sum()

## Exploratory Data Analysis

for col in df.select_dtypes(include=['float','int']).columns:
  sns.histplot(df[col], kde=True)
  plt.title(f" Histogram of {col} ")
  plt.xlabel(col)
  plt.ylabel("Frequency")
  plt.show()

for col in df.select_dtypes(include=['object']).columns:
  sns.barplot(df[col])
  plt.title(f" Bar Chart of {col} ")
  plt.xlabel(col)
  plt.ylabel("Frequency")
  plt.show()

## Bivariate Analysis

num = df.select_dtypes(include=['float','int']).columns

for i in range(len(num)):
  for j in range(i+1, len(num)):
    sns.scatterplot(data=df, x=num[i], y=num[j])
    plt.xlabel(num[i])
    plt.ylabel(num[j])
    plt.show()

## Handling Outliers

for column in df.select_dtypes(include=['float64','int64']):
  sns.boxplot(df[column])
  plt.show()


for column in df.select_dtypes(include = "number"):
  q1 = df[column].quantile(0.25)
  q3 = df[column].quantile(0.75)
  iqr = q3-q1
  lower = q1 - 1.5*iqr
  upper = q3 + 1.5* iqr
  df[column] = df[column].clip(lower = lower, upper= upper)

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

le = LabelEncoder()

for col in df.select_dtypes(include=["object"]).columns:
  df[col] = le.fit_transform(df[col])

df.head()

correlation_matrix = df.corr()

correlation_matrix

plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

sse = [] # The sum of Squared Errors =SSE
k_rng = range(1,10)
for k in k_rng:
   km = KMeans(n_clusters=k)
   km.fit(df[['income','recency']])
   sse.append(km.inertia_)

plt.xlabel('K')
plt.ylabel('Sum of squared error')
plt.plot(k_rng,sse)

km = KMeans(n_clusters=3)
y_predicted = km.fit_predict(df[['income','recency']])
y_predicted
df['cluster']=y_predicted
df.head(25)
print(km.cluster_centers_)

df1 = df[df.cluster==0]
df2 = df[df.cluster==1]
df3 = df[df.cluster==2]
plt.scatter(df1.income,df1['recency'],color='green')
plt.scatter(df2.income,df2['recency'],color='red')
plt.scatter(df3.income,df3['recency'],color='black')
plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')
plt.xlabel('income')
plt.ylabel('recency')
plt.legend()

silhouette_score(df, y_predicted)